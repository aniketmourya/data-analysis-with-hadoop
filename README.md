# web_crawling-hadoop-data_analysis
The Hadoop  platform  has been  widely used for large-scale data processing and analysis recently. Log Analytics is a technique for automatically understanding the meaningful patterns from heterogeneous data. Hadoop is most popular for processing Big Data. Big data is nothing but collection of huge amount data which is not possible to process with traditional system .Big data can be categorized into three format firstly structured data, second semi structured data and third unstructured data. So structured data can be stored into relational database and semi structured data can be stored into xml file and unstructured data can be stored into text,excel and PDF file. With traditional system this not possible to process unstructured data which is excel format to solve this problem the project is implementing hadoop technology to process this unstructured data. The project is processing large unstructured retail domain web  crawling dataset which is excel format using one of the hadoop component i.e mapreduce, after processing this data is stored on HDFS in multiple file according to different condition for analysis using dashboard. After using hive component of hadoop it fires advanced query on this dataset and using sqoop one of the hadoop component it exports this data into relational database and  perform Prediction and Classification over the web crawling data.
